{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register and Validate Model\n",
    " - Register Model and set tag to {\"validation_status\":\"pending\"}\n",
    " - Validate model, set alias to production and chnage tag to validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Dynamically add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validations.validate import RegisterEvaluateModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get enviornment variables\n",
    "dbutils.widgets.text(\"env\", \"dev\")\n",
    "dbutils.widgets.text(\"model_name\", \"dev_occupancymodel\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"devaiml\")\n",
    "dbutils.widgets.text(\"schema_name\", \"occupancy_project\")\n",
    "dbutils.widgets.text(\"experiment_name\", \"dev_ProjectOccupancy\")\n",
    "dbutils.widgets.text(\"validation_data_path\", \"dbfs:/FileStore/ValidationData/occupancy_mlops_validation_data\")\n",
    "\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")\n",
    "validation_data_path = dbutils.widgets.get(\"validation_data_path\")\n",
    "# get best run_id that can be inferred from the job\n",
    "best_run_id = dbutils.jobs.taskValues.get(\"Train-Model\",\"best_run_id\", debugValue=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # define model name with 3 name space\n",
    "    registered_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "    # get username from databricks notebook context\n",
    "    username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "    try:\n",
    "        # start model model evaluation\n",
    "        evaluate = RegisterEvaluateModel(\n",
    "            registered_model_name,\n",
    "            experiment_name,\n",
    "            username,\n",
    "            validation_data_path\n",
    "        )\n",
    "        # register the model\n",
    "        model_info = evaluate.register_model(best_run_id)\n",
    "        # evaluate and assign alias\n",
    "        evaluate.evaluate_model(model_info)\n",
    "        logging.info(f\"Model Evaluation completed for: {registered_model_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to complete {registered_model_name} Evaluation\")\n",
    "        raise RuntimeError(f\"Error during model evaluation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
